{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNet w/ RPN Heads for multi-tasking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_test = torchvision.models.squeezenet1_1(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_test.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fire module, in the forward function, is already concatenating the 1x1 and 3x3 expand layers. If we call on the Fire function or squeezenet, we should be able to use it.\n",
    "In the github repo, they said that the final layer input is innitialized weirdly so now, I have to figure out how they do it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cat([model_test.features[12].expand1x1.weight[0],\n",
    "model_test.features[12].expand3x3.weight[0]], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_layer = model_test.features[12].expand1x1.weight"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "final_layer[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "from torch.utils.model_zoo import load_url as load_state_dict_from_url\n",
    "\n",
    "__all__ = ['SqueezeNet', 'squeezenet1_0', 'squeezenet1_1']\n",
    "\n",
    "model_urls = {\n",
    "    'squeezenet1_0': 'https://download.pytorch.org/models/squeezenet1_0-a815701f.pth',\n",
    "    'squeezenet1_1': 'https://download.pytorch.org/models/squeezenet1_1-f364aa15.pth',\n",
    "}\n",
    "\n",
    "\n",
    "class Fire(nn.Module):\n",
    "\n",
    "    def __init__(self, inplanes, squeeze_planes,\n",
    "                 expand1x1_planes, expand3x3_planes):\n",
    "        super(Fire, self).__init__()\n",
    "        self.inplanes = inplanes\n",
    "        self.squeeze = nn.Conv2d(inplanes, squeeze_planes, kernel_size=1)\n",
    "        self.squeeze_activation = nn.ReLU(inplace=True)\n",
    "        self.expand1x1 = nn.Conv2d(squeeze_planes, expand1x1_planes,\n",
    "                                   kernel_size=1)\n",
    "        self.expand1x1_activation = nn.ReLU(inplace=True)\n",
    "        self.expand3x3 = nn.Conv2d(squeeze_planes, expand3x3_planes,\n",
    "                                   kernel_size=3, padding=1)\n",
    "        self.expand3x3_activation = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.squeeze_activation(self.squeeze(x))\n",
    "        final_conv = torch.cat([\n",
    "            self.expand1x1_activation(self.expand1x1(x)),\n",
    "            self.expand3x3_activation(self.expand3x3(x))\n",
    "        ], 1)\n",
    "        return final_conv\n",
    "\n",
    "\n",
    "class SqueezeNet(nn.Module):\n",
    "\n",
    "    def __init__(self, version='1_0', num_classes=1000):\n",
    "        super(SqueezeNet, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        if version == '1_0':\n",
    "            self.features = nn.Sequential(\n",
    "                nn.Conv2d(3, 96, kernel_size=7, stride=2),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
    "                Fire(96, 16, 64, 64),\n",
    "                Fire(128, 16, 64, 64),\n",
    "                Fire(128, 32, 128, 128),\n",
    "                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
    "                Fire(256, 32, 128, 128),\n",
    "                Fire(256, 48, 192, 192),\n",
    "                Fire(384, 48, 192, 192),\n",
    "                Fire(384, 64, 256, 256),\n",
    "                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
    "                Fire(512, 64, 256, 256),\n",
    "            )\n",
    "        elif version == '1_1':\n",
    "            self.features = nn.Sequential(\n",
    "                nn.Conv2d(3, 64, kernel_size=3, stride=2),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
    "                Fire(64, 16, 64, 64),\n",
    "                Fire(128, 16, 64, 64),\n",
    "                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
    "                Fire(128, 32, 128, 128),\n",
    "                Fire(256, 32, 128, 128),\n",
    "                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
    "                Fire(256, 48, 192, 192),\n",
    "                Fire(384, 48, 192, 192),\n",
    "                Fire(384, 64, 256, 256),\n",
    "                Fire(512, 64, 256, 256),\n",
    "            )\n",
    "        else:\n",
    "            # FIXME: Is this needed? SqueezeNet should only be called from the\n",
    "            # FIXME: squeezenet1_x() functions\n",
    "            # FIXME: This checking is not done for the other models\n",
    "            raise ValueError(\"Unsupported SqueezeNet version {version}:\"\n",
    "                             \"1_0 or 1_1 expected\".format(version=version))\n",
    "\n",
    "        # Final convolution is initialized differently from the rest\n",
    "        final_conv = nn.Conv2d(512, self.num_classes, kernel_size=1)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(p=0.5),\n",
    "            final_conv,\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.AdaptiveAvgPool2d((1, 1))\n",
    "        )\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                if m is final_conv:\n",
    "                    init.normal_(m.weight, mean=0.0, std=0.01)\n",
    "                else:\n",
    "                    init.kaiming_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    init.constant_(m.bias, 0)\n",
    "# would probably have to do something here \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return torch.flatten(x, 1)\n",
    "\n",
    "\n",
    "def _squeezenet(version, pretrained, progress, **kwargs):\n",
    "    model = SqueezeNet(version, **kwargs)\n",
    "    if pretrained:\n",
    "        arch = 'squeezenet' + version\n",
    "        state_dict = load_state_dict_from_url(model_urls[arch],\n",
    "                                              progress=progress)\n",
    "        model.load_state_dict(state_dict)\n",
    "    return model\n",
    "\n",
    "\n",
    "def squeezenet1_0(pretrained=False, progress=True, **kwargs):\n",
    "    r\"\"\"SqueezeNet model architecture from the `\"SqueezeNet: AlexNet-level\n",
    "    accuracy with 50x fewer parameters and <0.5MB model size\"\n",
    "    <https://arxiv.org/abs/1602.07360>`_ paper.\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    return _squeezenet('1_0', pretrained, progress, **kwargs)\n",
    "\n",
    "\n",
    "def squeezenet1_1(pretrained=False, progress=True, **kwargs):\n",
    "    r\"\"\"SqueezeNet 1.1 model from the `official SqueezeNet repo\n",
    "    <https://github.com/DeepScale/SqueezeNet/tree/master/SqueezeNet_v1.1>`_.\n",
    "    SqueezeNet 1.1 has 2.4x less computation and slightly fewer parameters\n",
    "    than SqueezeNet 1.0, without sacrificing accuracy.\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    return _squeezenet('1_1', pretrained, progress, **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = squeezenet1_1(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SqueezeNet(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2))\n",
       "    (1): ReLU(inplace)\n",
       "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "    (3): Fire(\n",
       "      (squeeze): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace)\n",
       "      (expand1x1): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace)\n",
       "      (expand3x3): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace)\n",
       "    )\n",
       "    (4): Fire(\n",
       "      (squeeze): Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace)\n",
       "      (expand1x1): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace)\n",
       "      (expand3x3): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace)\n",
       "    )\n",
       "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "    (6): Fire(\n",
       "      (squeeze): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace)\n",
       "      (expand1x1): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace)\n",
       "      (expand3x3): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace)\n",
       "    )\n",
       "    (7): Fire(\n",
       "      (squeeze): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace)\n",
       "      (expand1x1): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace)\n",
       "      (expand3x3): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace)\n",
       "    )\n",
       "    (8): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "    (9): Fire(\n",
       "      (squeeze): Conv2d(256, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace)\n",
       "      (expand1x1): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace)\n",
       "      (expand3x3): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace)\n",
       "    )\n",
       "    (10): Fire(\n",
       "      (squeeze): Conv2d(384, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace)\n",
       "      (expand1x1): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace)\n",
       "      (expand3x3): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace)\n",
       "    )\n",
       "    (11): Fire(\n",
       "      (squeeze): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace)\n",
       "      (expand1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace)\n",
       "      (expand3x3): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace)\n",
       "    )\n",
       "    (12): Fire(\n",
       "      (squeeze): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace)\n",
       "      (expand1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace)\n",
       "      (expand3x3): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace)\n",
       "    )\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.5)\n",
       "    (1): Conv2d(512, 1000, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (2): ReLU(inplace)\n",
       "    (3): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for param in model.parameters():\n",
    "    print(len(param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note\n",
    "Look what to put in for the conv layer features from the SqueezeNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv8_2 = nn.Conv2d(256, 512, kernel_size=3, stride=2, padding=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't necessarily need them \n",
    "class AuxiliaryConvolutions(nn.Module):\n",
    "    \"\"\"\n",
    "    Additional Convolutions to produce higher-level feature maps.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(AuxiliaryConvolutions, self).__init__()\n",
    "        self.conv8_1 = nn.Conv2d(1024, 256, kernel_size=1, padding=0)\n",
    "        self.conv8_2 = nn.Conv2d(256, 512, kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        self.conv9_1 = nn.Conv2d(512, 128, kernel_size=1, padding=0)\n",
    "        self.conv9_2 = nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        self.conv10_1 = nn.Conv2d(256, 128, kernel_size=1, padding=0)\n",
    "        self.conv10_2 = nn.Conv2d(128, 256, kernel_size=3, padding=0)\n",
    "\n",
    "        self.conv11_1 = nn.Conv2d(256, 128, kernel_size=1, padding=0)\n",
    "        self.conv11_2 = nn.Conv2d(128, 256, kernel_size=3, padding=0)\n",
    "            \n",
    "            # initialize the new parameters\n",
    "            \n",
    "        self.init_conv2d()\n",
    "    \n",
    "    def init_conv2d(self):\n",
    "        \"\"\"\n",
    "        Initialize convolution parameters\n",
    "        \"\"\"\n",
    "        \n",
    "        for c in self.children():\n",
    "            if isinstance(c, nn.Conv2d):\n",
    "                nn.init.xavier_uniform_(c.weight)\n",
    "                nn.init.constant_(c.bias, 0.)\n",
    "                \n",
    "    def forward(self, conv7_feats):\n",
    "        \"\"\"\n",
    "        Forward the new Auxiliary conv layers\n",
    "        \"\"\"\n",
    "        x = F.relu(self.conv8_1(model_test.features[12].expand1x1.weight))\n",
    "        x= F.relu(self.conv8_2(x))\n",
    "        conv8_2_feats = x\n",
    "        \n",
    "        x = F.relu(self.conv9_1(x))\n",
    "        x = F.relu(self.conv9_2(x))\n",
    "        conv9_2_feats = x\n",
    "        \n",
    "        x = F.relu(self.conv10_1(x))\n",
    "        x = F.relu(self.conv10_2(x))\n",
    "        conv10_2_feats = x\n",
    "        \n",
    "        x = F.relu(self.conv11_1(x))\n",
    "        conv11_2_feats = F.relu(self.conv11_s(x))\n",
    "        \n",
    "        return conv8_2_feats, conv9_2_feats, conv10_2_feats, conv11_2_feats\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_aux = AuxiliaryConvolutions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_aux.conv8_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Important Part: Prediction Convolutions\n",
    "\n",
    "Convolutions for predicting class scores nad bounding boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredictionConvolutions(nn.Module):\n",
    "    \"\"\"\n",
    "    Convolutions to predict the class scores and bounding boxes using lower and higher-level feature maps.\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, n_classes):\n",
    "        \"\"\"\n",
    "        n_classes: Number of different types of objects\n",
    "        \"\"\"\n",
    "        super(PredictionConvolutions, self).__init__()\n",
    "        self.n_classes = n_classes \n",
    "        \n",
    "        # since we want the boxes from multiple different feature maps\n",
    "        # we will define the specific layers we want the boxes from\n",
    "        n_boxes = {'conv8_2': 6,    # 8_2 and 9_2 are going to have 2 extra boxes with 3:1 and 1:3 aspect ratios\n",
    "                   'conv9_2': 6,    # because the feature maps are huge from these layers\n",
    "                   'conv10_2':4,\n",
    "                   'conv11_2':4}\n",
    "        \n",
    "        # the four layers that we want the prior boxes from \n",
    "        # then, we are going to stack the feature maps on top of each other\n",
    "        self.loc_conv8_2 = nn.Conv2d(512, n_boxes['conv8_2'] * 4, kernel_size = 3, padding=1)\n",
    "        self.loc_conv9_2 = nn.Conv2d(256, n_boxes['conv9_2'] * 4, kernel_size = 3, padding =1)\n",
    "        self.loc_conv10_2 = nn.Conv2d(256, n_boxes['conv10_2'] * 4, kernel_size = 3, padding =1)\n",
    "        self.loc_conv11_2 = nn.Conv2d(256, n_boxes['conv11_2'] * 4, kernel_size=3, padding=1)\n",
    "        \n",
    "        # class probabilities predictions\n",
    "        # for all the feature maps we have prior boxes for, we also want the class predictions\n",
    "        self.cl_conv8_2 = nn.Conv2d(512, n_boxes['conv8_2'] * n_classes, kernel_size=3, padding=1)\n",
    "        self.cl_conv9_2 = nn.Conv2d(256, n_boxes['conv9_2'] * n_classes, kernel_size=3, padding=1)\n",
    "        self.cl_conv10_2 = nn.Conv2d(256, n_boxes['conv10_2'] * n_classes, kernel_size=3, padding=1)\n",
    "        self.cl_conv11_2 = nn.Conv2d(256, n_boxes['conv11_2'] * n_classes, kernel_size=3, padding=1)\n",
    "        \n",
    "        # initialize the weights\n",
    "        \n",
    "        self.init_conv2d()\n",
    "        \n",
    "    def init_conv2d(self):\n",
    "        \"\"\"\n",
    "        Simple function to initialize weights\n",
    "        \"\"\"\n",
    "        for c in self.children():\n",
    "            if isinstance(c, nn.Conv2d):\n",
    "                nn.init.xavier_uniform_(c.weight)\n",
    "                nn.init.constant_(c.bias, 0.)\n",
    "                \n",
    "    \n",
    "    # let's define the forward functions:\n",
    "    \n",
    "    def forward(self, conv8_2, conv9_2, conv10_2, conv11_2):\n",
    "        \"\"\"\n",
    "        Takes in the layers we defined in axuiliary convolutions and forwards them while getting the boxes outputs too\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        batch_size = conv8_2_feats.size(0)   # just taking a number from the feature map output by conv8_2 layer\n",
    "        # seems like we want the number of the boxes output by the conv8_2 layer to be the same \n",
    "        \n",
    "        # let's start with the first layer of the auxiliary convolutions:\n",
    "        \n",
    "        # perform the convolution on the feature map of the layer conv8_2 and get the output\n",
    "        l_conv8_2 = self.loc_conv8_2(conv8_2_feats)    # Outputs: (N, 24, 10, 10)\n",
    "        # now transform that output to be the same as prior-box order:\n",
    "        # permute just switches the location of values in the tensor\n",
    "        l_conv8_2 = l_conv8_2.permute(0, 2, 3, 1).contiguous() # (N, 10, 10, 24)\n",
    "        # .contiguous()Returns a contiguous in memory tensor containing the same data as self tensor. \n",
    "        # If self tensor is already in the specified memory format, this function returns the self tensor.\n",
    "        l_conv8_2 = l_conv8_2.view(batch_size, -1, 4) # \n",
    "        \n",
    "        l_conv9_2 = self.loc_conv8_2(conv9_2_feats)    # Outputs: (N, 24, 10, 10)\n",
    "        l_conv9_2 = l_conv9_2.permute(0, 2, 3, 1).contiguous() # (N, 10, 10, 24)\n",
    "        l_conv9_2 = l_conv9_2.view(batch_size, -1, 4) # (N, 150, 4)\n",
    "        \n",
    "        l_conv10_2 = self.loc_conv8_2(conv10_2_feats)    # Outputs: (N, 24, 10, 10)\n",
    "        l_conv10_2 = l_conv10_2.permute(0, 2, 3, 1).contiguous() # (N, 10, 10, 24)\n",
    "        l_conv10_2 = l_conv10_2.view(batch_size, -1, 4) # (N, 36, 4)\n",
    "        \n",
    "        l_conv11_2 = self.loc_conv8_2(conv11_2_feats)    # Outputs: (N, 24, 10, 10)\n",
    "        l_conv11_2 = l_conv11_2.permute(0, 2, 3, 1).contiguous() # (N, 10, 10, 24)\n",
    "        l_conv11_2 = l_conv11_2.view(batch_size, -1, 4) # (N, 4, 4)\n",
    "        \n",
    "        # Classification\n",
    "        # Predict classes for boxes i.e. Classification\n",
    "        c_conv8_2 = self.cl_conv8_2(conv8_2_feats)\n",
    "        c_conv8_2 = c_conv8_2.permute(0, 2, 3, 1).contiguous()\n",
    "        c_conv8_2 = c_conv8_2.view(batch_size, -1, self.n_classes)\n",
    "        \n",
    "        c_conv9_2 = self.cl_conv9_2(conv9_2_feats)\n",
    "        c_conv9_2 = c_conv9_2.permute(0, 2, 3, 1).contiguous()\n",
    "        c_conv9_2 = c_conv9_2.view(batch_size, -1, self.n_classes)\n",
    "        \n",
    "        c_conv10_2 = self.cl_conv10_2(conv10_2_feats)\n",
    "        c_conv10_2 = c_conv10_2.permute(0, 2, 3, 1).contiguous()\n",
    "        c_conv10_2 = c_conv10_2.view(batch_size, -1, self.n_classes)\n",
    "        \n",
    "        c_conv11_2 = self.cl_conv11_2(conv11_2_feats)\n",
    "        c_conv11_2 = c_conv11_2.permute(0, 2, 3, 1).contiguous()\n",
    "        c_conv11_2 = c_conv11_2.view(batch_size, -1, self.n_classes)\n",
    "        \n",
    "        locs = torch.cat([l_conv8_2, l_conv9_2, l_conv10_2, l_conv11_2], dim=1)\n",
    "        class_scores = torch.cat([c_conv8_2, c_conv9_2, c_conv10_2, c_conv11_2])\n",
    "        \n",
    "        return locs, class_scores\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now comes the fun part where we try to get the network put together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Squeeze_detect(nn.Module):\n",
    "    \"\"\"\n",
    "    squeeze_detect network that encapsulates the base Squeezenet, auxiliary and prediction convs\n",
    "    \"\"\"\n",
    "    def __init__(self, n_classes):\n",
    "        super(Squeeze_detect, self).__init__()\n",
    "        self.n_classes = n_classes\n",
    "        \n",
    "        self.base = SqueezeNet()\n",
    "        self.aux_convs = AuxiliaryConvolutions()\n",
    "        self.pred_convs = PredictionConvolutions(n_classes)\n",
    "        \n",
    "        self.rescale_factors = nn.Parameter(torch.FloatTensor(1, 512, 1, 1))    # there are 512 channels in 8_2 \n",
    "        nn.init.constant_(self.rescale_factors, 20)\n",
    "        \n",
    "        # prior boxes \n",
    "        self.prior_cxcy = self.create_prior_boxes()\n",
    "        \n",
    "    def forward(self, image):\n",
    "        \"\"\"\n",
    "        Forward Propagation.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Do not need to do these because we are not really retraining stuff from squeezenet\n",
    "       # conv8_2_feats = self.base(image)\n",
    "        \n",
    "        # rescale the conv8_2 after L2 norm\n",
    "       # norm = conv8_2_feats.pow(2).sum(dim=1, keepdim=True).sqrt()\n",
    "        # conv8_2_feats = conv8_2_feats / norm\n",
    "        # conv8_2_feats = conv8_3_feats * self.rescale_factors\n",
    "        \n",
    "        # Auxiliary\n",
    "        # higher level feature map generators\n",
    "        # HERE it would be something like SqueezeNet.features[12].expand1x1.weights?\n",
    "        conv8_2_feats, conv9_2_feats, conv10_2_feats, conv11_2_feats = self.aux_convs(conv7_feats)\n",
    "        \n",
    "        # run prediction convolutions\n",
    "        locs, classes_scores = self.pred_convs(conv8_2_feats, conv9_2_feats, conv10_2_feats, conv11_2_feats)\n",
    "        \n",
    "        return locs, class_scores\n",
    "    \n",
    "    def create_prior_boxes(self):\n",
    "        \"\"\"\n",
    "        create the ~ 8K numbers of priors but they would be less because we have less layers that we are doing\n",
    "        calcs on\n",
    "        \"\"\"\n",
    "        \n",
    "        # the feature map dimensions for each layer, we want the end \n",
    "        fmap_dims = {'conv8_2': 10,\n",
    "                     'conv9_2':5,\n",
    "                     'conv10_2':3,\n",
    "                     'conv11_2':1\n",
    "                    }\n",
    "    \n",
    "        obj_scales = {'conv8_2':0.375,\n",
    "                      'conv9_2':0.55,\n",
    "                      'conv10_2':0.725,\n",
    "                      'conv11_2':0.9                     \n",
    "                     }\n",
    "        aspect_ratios = {'conv8_2': [1., 2., 3., 0.5, .333],\n",
    "                         'conv9_2':[1., 2., 3., 0.5, .333],\n",
    "                         'conv10_2':[1., 2., 0.5],\n",
    "                         'conv11_2':[1., 2., 0.5]}\n",
    "    \n",
    "        \n",
    "        fmaps = list(fmap_dims.keys())\n",
    "        \n",
    "        prior_boxes = []\n",
    "        \n",
    "        for k, fmap in enumerate(fmaps):\n",
    "            for i in range(fmaps_dims[fmap]):\n",
    "                for j in range(fmaps_dims[fmap]):\n",
    "                    cx = (j + 0.5) / fmap_dims[fmap]\n",
    "                    cy = (i + 0.5) / fmap_dims[fmap]\n",
    "                    \n",
    "                    for ratio in aspect_ratios[fmap]:\n",
    "                        prior_boxes.append([cx, cy, obj_scales[fmap] * sqrt(ratio), obj_scales[fmap] /sqrt(ratio)])\n",
    "                        \n",
    "                        if ratio == 1:\n",
    "                            try:\n",
    "                                additional_scale = sqrt(obj_scales[fmap] * obj_scales[fmaps[k+1]])\n",
    "                                \n",
    "                            except:\n",
    "                                additional_scale = 1.\n",
    "                                prior_boxes.append([cx, cy, additional_scale, additional_scale])\n",
    "                                \n",
    "        prior_boxes = torch.FloatTensor(prior_boxes).to(device)\n",
    "        prior_boxes.clamp_(0, 1)\n",
    "        \n",
    "        return prior_boxes\n",
    "    \n",
    "    \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "        fmap_dims = {'conv8_2': 10,\n",
    "                     'conv9_2':5,\n",
    "                     'conv10_2':3,\n",
    "                     'conv11_2':1\n",
    "                    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "    fmaps = list(fmap_dims.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "conv8_2\n",
      "1\n",
      "conv9_2\n",
      "2\n",
      "conv10_2\n",
      "3\n",
      "conv11_2\n"
     ]
    }
   ],
   "source": [
    "for k, fmap in enumerate(fmaps):\n",
    "    print(k)\n",
    "    print(fmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl = PredictionConvolutions(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we put nn.ReLU in between the conv layers, we would call on nn.Module and type:\n",
    "nn.ReLU(inplace=True)\n",
    "\n",
    "BUT if we put in the forward function, we would call on the ReLU function directly and "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IncompatibleKeys(missing_keys=[], unexpected_keys=[])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('squeezenet1_0-a815701f.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SqueezeNet(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 96, kernel_size=(7, 7), stride=(2, 2))\n",
       "    (1): ReLU(inplace)\n",
       "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "    (3): Fire(\n",
       "      (squeeze): Conv2d(96, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace)\n",
       "      (expand1x1): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace)\n",
       "      (expand3x3): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace)\n",
       "    )\n",
       "    (4): Fire(\n",
       "      (squeeze): Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace)\n",
       "      (expand1x1): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace)\n",
       "      (expand3x3): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace)\n",
       "    )\n",
       "    (5): Fire(\n",
       "      (squeeze): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace)\n",
       "      (expand1x1): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace)\n",
       "      (expand3x3): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace)\n",
       "    )\n",
       "    (6): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "    (7): Fire(\n",
       "      (squeeze): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace)\n",
       "      (expand1x1): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace)\n",
       "      (expand3x3): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace)\n",
       "    )\n",
       "    (8): Fire(\n",
       "      (squeeze): Conv2d(256, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace)\n",
       "      (expand1x1): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace)\n",
       "      (expand3x3): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace)\n",
       "    )\n",
       "    (9): Fire(\n",
       "      (squeeze): Conv2d(384, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace)\n",
       "      (expand1x1): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace)\n",
       "      (expand3x3): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace)\n",
       "    )\n",
       "    (10): Fire(\n",
       "      (squeeze): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace)\n",
       "      (expand1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace)\n",
       "      (expand3x3): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace)\n",
       "    )\n",
       "    (11): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "    (12): Fire(\n",
       "      (squeeze): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace)\n",
       "      (expand1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace)\n",
       "      (expand3x3): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace)\n",
       "    )\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.5)\n",
       "    (1): Conv2d(512, 1000, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (2): ReLU(inplace)\n",
       "    (3): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DETR model head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the squeezenet backbone, passing the image through the CNN, we pass in the feature pyramid to transformer encoder-decoder, which outputs a box of predictions:\n",
    "\n",
    "1. CNN Backbone:\n",
    "    - Input: Image\n",
    "    - Output: Feature maps with multiple channels that are flattened to be passed into the encoder\n",
    "2. Transformer encoder-decoder:\n",
    "    - Encoder takes in the flattened layer and outputs the same size shape sequence.\n",
    "    - Decoder takes in the output from the encoder but also takes in Object Queries\n",
    "    #### Object Queries\n",
    "        - Big N, represents the output we want i.e. if 4 tuples for 4 objects and their bounding boxes, we get that.\n",
    "        - Each object query vector wouldask the image different questions like what is in your right box, left box, lower left box, lower right box.\n",
    "3. Feed Forwad Network (FFN):\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
