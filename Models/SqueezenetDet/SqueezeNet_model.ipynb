{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SqueezeNet with YOLO-like detection head\n",
    "\n",
    "ConvDet is the layer that performs the object detection, outputting bounding boxes as well as the classification scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_test = torchvision.models.squeezenet1_1(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_test.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fire module, in the forward function, is already concatenating the 1x1 and 3x3 expand layers. If we call on the Fire function or squeezenet, we should be able to use it.\n",
    "In the github repo, they said that the final layer input is innitialized weirdly so now, I have to figure out how they do it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_layer = model_test.features[12].expand1x1.weight"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "final_layer[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "from torch.utils.model_zoo import load_url as load_state_dict_from_url\n",
    "\n",
    "__all__ = ['SqueezeNet', 'squeezenet1_0', 'squeezenet1_1']\n",
    "\n",
    "model_urls = {\n",
    "    'squeezenet1_0': 'https://download.pytorch.org/models/squeezenet1_0-a815701f.pth',\n",
    "    'squeezenet1_1': 'https://download.pytorch.org/models/squeezenet1_1-f364aa15.pth',\n",
    "}\n",
    "\n",
    "\n",
    "class Fire(nn.Module):\n",
    "\n",
    "    def __init__(self, inplanes, squeeze_planes,\n",
    "                 expand1x1_planes, expand3x3_planes):\n",
    "        super(Fire, self).__init__()\n",
    "        self.inplanes = inplanes\n",
    "        self.squeeze = nn.Conv2d(inplanes, squeeze_planes, kernel_size=1)\n",
    "        self.squeeze_activation = nn.ReLU(inplace=True)\n",
    "        self.expand1x1 = nn.Conv2d(squeeze_planes, expand1x1_planes, kernel_size=1)\n",
    "        self.expand1x1_activation = nn.ReLU(inplace=True)\n",
    "        self.expand3x3 = nn.Conv2d(squeeze_planes, expand3x3_planes, kernel_size=3, padding=1)\n",
    "        self.expand3x3_activation = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.squeeze_activation(self.squeeze(x))\n",
    "        final_conv = torch.cat([\n",
    "            self.expand1x1_activation(self.expand1x1(x)),\n",
    "            self.expand3x3_activation(self.expand3x3(x))\n",
    "        ], 1)\n",
    "        return final_conv\n",
    "\n",
    "\n",
    "class SqueezenetDet(nn.Module):\n",
    "\n",
    "    def __init__(self, cfg):\n",
    "        super(SqueezenetDet, self).__init__()\n",
    "        self.num_classes = cfg.num_classes   # we get the number of classes and anchors\n",
    "        self.num_anchors = cfg.num_anchors   # from the configs file\n",
    "        \n",
    "        if cfg.arch == 'squeezedet':\n",
    "            self.features = nn.Sequential(\n",
    "                nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
    "                Fire(64, 16, 64, 64),\n",
    "                Fire(128, 16, 64, 64),\n",
    "                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
    "                Fire(128, 32, 128, 128),\n",
    "                Fire(256, 32, 128, 128),\n",
    "                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
    "                Fire(256, 48, 192, 192),\n",
    "                Fire(384, 48, 192, 192),\n",
    "                Fire(384, 64, 256, 256),\n",
    "                Fire(512, 64, 256, 256),\n",
    "                Fire(512, 96, 384, 384),\n",
    "                Fire(768, 96, 384, 384)\n",
    "            )\n",
    "        elif cfg.arch == 'squeezedetplus':\n",
    "            self.features = nn.Sequential(\n",
    "                nn.Conv2d(3, 96, kernel_size=7, stride=2, padding=3),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
    "                Fire(96, 96, 64, 64),\n",
    "                Fire(128, 96, 64, 64),\n",
    "                Fire(128, 192, 128, 128),\n",
    "                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
    "                Fire(256, 192, 128, 128),\n",
    "                Fire(256, 288, 192, 192),\n",
    "                Fire(384, 288, 192, 192),\n",
    "                Fire(384, 384, 256, 256),\n",
    "                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
    "                Fire(512, 384, 256, 256),\n",
    "                Fire(512, 384, 256, 256),\n",
    "                Fire(512, 384, 256, 256),\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported SqueezeNet version\")\n",
    "\n",
    "        # adding a drop out layer, might get rid of it later on\n",
    "        self.dropout = nn.Dropout(cfg.dropoout_prob, inplace=True) \\\n",
    "            if cfg.dropout_prob > 0 else None\n",
    "        self.convdet = nn.Conv2d(768 if cfg.arch == 'squeezedet' else 512,\n",
    "                                 cfg.anchors_per_grid * (cfg.num_classes + 5),  # K (n_classes + 5) from the SqueezeDet paper +1 is for confidence\n",
    "                                 kernel_size=3, padding=1)                      # score of how likely it is that the object exists in the box\n",
    "        \n",
    "        self.init_weights()\n",
    "\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        if self.dropout is not None:\n",
    "            x = self.dropout(x)\n",
    "        x = self.convdet(x)\n",
    "        x = x.permute(0, 2, 3, 1).contiguous()          # already doing that with transform so let's compare\n",
    "        return x.view(-1, self.num_anchors, self.num_classes + 5)\n",
    "        \n",
    "\n",
    "    def init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                if m is self.convdet:\n",
    "                    nn.init.normal_(m.weight, mean=0.0, std=0.002)\n",
    "                else:\n",
    "                    nn.init.normal_(m.weight, mean=0.0, std=0.005)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "# would probably have to do something here \n",
    "\n",
    "# prediction resolver i.e. make them interpretable\n",
    "\n",
    "def safe_softmax(probs, dim=None):\n",
    "    exp = torch.exp(probs - torch.max(probs, dim=dim, keepdim=True)[0])\n",
    "    return exp / torch.sum(exp, dim=dim, keepdim=True)\n",
    "\n",
    "def detlas_to_boxes(deltas, anchors, input_size):\n",
    "    \"\"\"\n",
    "    :params deltas: xywh format\n",
    "    :params anchorsL xywh format\n",
    "    :params input_size: input image size in hw format\n",
    "    :return: boxes in xyxy format\n",
    "    \"\"\"\n",
    "    boxes_xywh = torch.cat([\n",
    "        anchors[..., [0]] + anchors[..., [2]] * deltas[..., [0]],\n",
    "        anchors[..., [1]] + anchors[..., [3]] * deltas[..., [1]],\n",
    "        anchors[..., [2]] * torch.exp(deltas[..., [2]]),\n",
    "        anchors[..., [3]] * torch.exp(deltas[..., [3]])\n",
    "    ], dim=2)\n",
    "    \n",
    "    boxes_xyxy = xywh_to_xyxy(boxes_xywh)\n",
    "    boxes_xyxy[..., [0, 2]] = torch.clamp(boxes_xyxy[..., [0,2]], 0, input_size[1] -1)\n",
    "    boxes_xyxy[..., [1, 3]] = torch.clamp(boxes_xyxy[..., [1,3]], 0, input_size[0] -1)\n",
    "    \n",
    "    return boxes_xyxy\n",
    "    \n",
    "def xyxy_to_xywh(boxes_xyxy):\n",
    "    assert torch.all(boxes_xyxy[..., 0] < boxes_xyxy[..., 2])\n",
    "    assert torch.all(boxes_xyxy[..., 1] < boxes_xyxy[..., 3])\n",
    "    return torch.cat([\n",
    "        (boxes_xyxy[..., [0]] + boxes_xyxy[..., [2]]) / 2.,\n",
    "        (boxes_xyxy[..., [1]] + boxes_xyxy[..., [3]]) / 2.,\n",
    "        boxes_xyxy[..., [2]] - boxes_xyxy[..., [0]] + 1.,\n",
    "        boxes_xyxy[..., [3]] - boxes_xyxy[..., [1]] + 1.\n",
    "    ], dim=-1)\n",
    "\n",
    "\n",
    "def xywh_to_xyxy(boxes_xywh):\n",
    "    assert torch.all(boxes_xywh[..., [2, 3]] > 0)\n",
    "    return torch.cat([\n",
    "        boxes_xywh[..., [0]] - 0.5 * (boxes_xywh[..., [2]] - 1),\n",
    "        boxes_xywh[..., [1]] - 0.5 * (boxes_xywh[..., [3]] - 1),\n",
    "        boxes_xywh[..., [0]] + 0.5 * (boxes_xywh[..., [2]] - 1),\n",
    "        boxes_xywh[..., [1]] + 0.5 * (boxes_xywh[..., [3]] - 1)\n",
    "    ], dim=-1)\n",
    "\n",
    "\n",
    "class PredictionResolver(nn.Module):\n",
    "    def __init__(self, cfg, log_softmax=False):\n",
    "        super(PredictionResolver, self).__init__()\n",
    "        \n",
    "        self.log_softmax = log_softmax\n",
    "        self.input_size = cfg.input_size\n",
    "        self.num_classes = cfg.num_classes\n",
    "        self.anchors = torch.from_numpy(cfg.anchors).unsqueeze(0).float()\n",
    "        self.anchors_per_grid = cfg.anchors_per_grid\n",
    "        \n",
    "        def forward(self, pred):\n",
    "            pred_class_probs = safe_softmax(pred[..., :self.num_classes].contiguous(), dim=-1)\n",
    "            pred_log_class_probs = None if not self.log_softmax else \\\n",
    "                torch.log_softmax(pred[..., :self.num_classes].contiguous(), dim=-1)   # this would not include the +1 for C so we will\n",
    "            # be fine because we will only have the number of probablities for all the classes expected\n",
    "            pred_scores = torch.sigmoid(pred[..., self.num_classes:self.num_classes + 1].contiguous())\n",
    "            pred_deltas = pred[..., self.num_classes + 1:].contiguous()\n",
    "            pred_boxes = deltas_to_boxes(pred_deltas, self.anchors.to(pred_deltas, device),\n",
    "                                         input_size=self.input_size)\n",
    "            return pred_class_probs, pred_log_class_probs, pred_scores, pred_deltas, pred_boxes\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "# Define the loss function next\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "def _squeezenet(version, pretrained, progress, **kwargs):\n",
    "    model = SqueezeNet(version, **kwargs)\n",
    "    if pretrained:\n",
    "        arch = 'squeezenet' + version\n",
    "        state_dict = load_state_dict_from_url(model_urls[arch],\n",
    "                                              progress=progress)\n",
    "        model.load_state_dict(state_dict)\n",
    "    return model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = squeezenet1_1(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SqueezeNet(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2))\n",
       "    (1): ReLU(inplace)\n",
       "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "    (3): Fire(\n",
       "      (squeeze): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace)\n",
       "      (expand1x1): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace)\n",
       "      (expand3x3): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace)\n",
       "    )\n",
       "    (4): Fire(\n",
       "      (squeeze): Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace)\n",
       "      (expand1x1): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace)\n",
       "      (expand3x3): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace)\n",
       "    )\n",
       "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "    (6): Fire(\n",
       "      (squeeze): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace)\n",
       "      (expand1x1): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace)\n",
       "      (expand3x3): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace)\n",
       "    )\n",
       "    (7): Fire(\n",
       "      (squeeze): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace)\n",
       "      (expand1x1): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace)\n",
       "      (expand3x3): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace)\n",
       "    )\n",
       "    (8): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "    (9): Fire(\n",
       "      (squeeze): Conv2d(256, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace)\n",
       "      (expand1x1): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace)\n",
       "      (expand3x3): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace)\n",
       "    )\n",
       "    (10): Fire(\n",
       "      (squeeze): Conv2d(384, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace)\n",
       "      (expand1x1): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace)\n",
       "      (expand3x3): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace)\n",
       "    )\n",
       "    (11): Fire(\n",
       "      (squeeze): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace)\n",
       "      (expand1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace)\n",
       "      (expand3x3): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace)\n",
       "    )\n",
       "    (12): Fire(\n",
       "      (squeeze): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace)\n",
       "      (expand1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace)\n",
       "      (expand3x3): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace)\n",
       "    )\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.5)\n",
       "    (1): Conv2d(512, 1000, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (2): ReLU(inplace)\n",
       "    (3): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'weight'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-f05adddc849b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0min_channels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'int' object has no attribute 'weight'"
     ]
    }
   ],
   "source": [
    "model.classifier[1].in_channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[[-0.0831]],\n",
       "\n",
       "         [[ 0.0611]],\n",
       "\n",
       "         [[-0.0077]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-0.0723]],\n",
       "\n",
       "         [[-0.0063]],\n",
       "\n",
       "         [[ 0.0907]]],\n",
       "\n",
       "\n",
       "        [[[-0.0974]],\n",
       "\n",
       "         [[-0.0289]],\n",
       "\n",
       "         [[-0.0833]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0.1137]],\n",
       "\n",
       "         [[ 0.1286]],\n",
       "\n",
       "         [[ 0.0308]]],\n",
       "\n",
       "\n",
       "        [[[-0.1306]],\n",
       "\n",
       "         [[-0.0525]],\n",
       "\n",
       "         [[ 0.1111]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0.0274]],\n",
       "\n",
       "         [[ 0.0665]],\n",
       "\n",
       "         [[-0.0765]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[-0.2096]],\n",
       "\n",
       "         [[ 0.0899]],\n",
       "\n",
       "         [[ 0.0486]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0.1198]],\n",
       "\n",
       "         [[ 0.1549]],\n",
       "\n",
       "         [[ 0.1210]]],\n",
       "\n",
       "\n",
       "        [[[-0.1408]],\n",
       "\n",
       "         [[-0.0767]],\n",
       "\n",
       "         [[-0.0139]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-0.0415]],\n",
       "\n",
       "         [[ 0.0364]],\n",
       "\n",
       "         [[-0.1884]]],\n",
       "\n",
       "\n",
       "        [[[-0.0546]],\n",
       "\n",
       "         [[-0.0680]],\n",
       "\n",
       "         [[-0.0690]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0.1427]],\n",
       "\n",
       "         [[-0.0874]],\n",
       "\n",
       "         [[-0.0630]]]], requires_grad=True)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.features[12].expand1x1.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64, 3, 3])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.features[12].expand3x3.weight[:1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "invalid argument 0: Sizes of tensors must match except in dimension 1. Got 1 and 3 in dimension 2 at ..\\aten\\src\\TH/generic/THTensor.cpp:711",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-db32939b6053>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m torch.cat((model.features[12].expand1x1.weight,\n\u001b[1;32m----> 2\u001b[1;33m model.features[12].expand3x3.weight), 1)\n\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m: invalid argument 0: Sizes of tensors must match except in dimension 1. Got 1 and 3 in dimension 2 at ..\\aten\\src\\TH/generic/THTensor.cpp:711"
     ]
    }
   ],
   "source": [
    "torch.cat((model.features[12].expand1x1.weight,\n",
    "model.features[12].expand3x3.weight), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for param in model.parameters():\n",
    "    print(len(param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note\n",
    "Look what to put in for the conv layer features from the SqueezeNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv8_2 = nn.Conv2d(256, 512, kernel_size=3, stride=2, padding=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't necessarily need them \n",
    "class AuxiliaryConvolutions(nn.Module):\n",
    "    \"\"\"\n",
    "    Additional Convolutions to produce higher-level feature maps.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(AuxiliaryConvolutions, self).__init__()\n",
    "        self.conv8_1 = nn.Conv2d(1024, 256, kernel_size=1, padding=0)\n",
    "        self.conv8_2 = nn.Conv2d(256, 512, kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        self.conv9_1 = nn.Conv2d(512, 128, kernel_size=1, padding=0)\n",
    "        self.conv9_2 = nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        self.conv10_1 = nn.Conv2d(256, 128, kernel_size=1, padding=0)\n",
    "        self.conv10_2 = nn.Conv2d(128, 256, kernel_size=3, padding=0)\n",
    "\n",
    "        self.conv11_1 = nn.Conv2d(256, 128, kernel_size=1, padding=0)\n",
    "        self.conv11_2 = nn.Conv2d(128, 256, kernel_size=3, padding=0)\n",
    "            \n",
    "            # initialize the new parameters\n",
    "            \n",
    "        self.init_conv2d()\n",
    "    \n",
    "    def init_conv2d(self):\n",
    "        \"\"\"\n",
    "        Initialize convolution parameters\n",
    "        \"\"\"\n",
    "        \n",
    "        for c in self.children():\n",
    "            if isinstance(c, nn.Conv2d):\n",
    "                nn.init.xavier_uniform_(c.weight)\n",
    "                nn.init.constant_(c.bias, 0.)\n",
    "                \n",
    "    def forward(self, conv7_feats):\n",
    "        \"\"\"\n",
    "        Forward the new Auxiliary conv layers\n",
    "        \"\"\"\n",
    "        x = F.relu(self.conv8_1(model_test.features[12].expand1x1.weight))\n",
    "        x= F.relu(self.conv8_2(x))\n",
    "        conv8_2_feats = x\n",
    "        \n",
    "        x = F.relu(self.conv9_1(x))\n",
    "        x = F.relu(self.conv9_2(x))\n",
    "        conv9_2_feats = x\n",
    "        \n",
    "        x = F.relu(self.conv10_1(x))\n",
    "        x = F.relu(self.conv10_2(x))\n",
    "        conv10_2_feats = x\n",
    "        \n",
    "        x = F.relu(self.conv11_1(x))\n",
    "        conv11_2_feats = F.relu(self.conv11_s(x))\n",
    "        \n",
    "        return conv8_2_feats, conv9_2_feats, conv10_2_feats, conv11_2_feats\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_aux = AuxiliaryConvolutions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_aux.conv8_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Important Part: Prediction Convolutions\n",
    "\n",
    "Convolutions for predicting class scores nad bounding boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredictionConvolutions(nn.Module):\n",
    "    \"\"\"\n",
    "    Convolutions to predict the class scores and bounding boxes using lower and higher-level feature maps.\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, n_classes):\n",
    "        \"\"\"\n",
    "        n_classes: Number of different types of objects\n",
    "        \"\"\"\n",
    "        super(PredictionConvolutions, self).__init__()\n",
    "        self.n_classes = n_classes \n",
    "        \n",
    "        # since we want the boxes from multiple different feature maps\n",
    "        # we will define the specific layers we want the boxes from\n",
    "        n_boxes = {'conv8_2': 6,    # 8_2 and 9_2 are going to have 2 extra boxes with 3:1 and 1:3 aspect ratios\n",
    "                   'conv9_2': 6,    # because the feature maps are huge from these layers\n",
    "                   'conv10_2':4,\n",
    "                   'conv11_2':4}\n",
    "        \n",
    "        # the four layers that we want the prior boxes from \n",
    "        # then, we are going to stack the feature maps on top of each other\n",
    "        self.loc_conv8_2 = nn.Conv2d(512, n_boxes['conv8_2'] * 4, kernel_size = 3, padding=1)\n",
    "        self.loc_conv9_2 = nn.Conv2d(256, n_boxes['conv9_2'] * 4, kernel_size = 3, padding =1)\n",
    "        self.loc_conv10_2 = nn.Conv2d(256, n_boxes['conv10_2'] * 4, kernel_size = 3, padding =1)\n",
    "        self.loc_conv11_2 = nn.Conv2d(256, n_boxes['conv11_2'] * 4, kernel_size=3, padding=1)\n",
    "        \n",
    "        # class probabilities predictions\n",
    "        # for all the feature maps we have prior boxes for, we also want the class predictions\n",
    "        self.cl_conv8_2 = nn.Conv2d(512, n_boxes['conv8_2'] * n_classes, kernel_size=3, padding=1)\n",
    "        self.cl_conv9_2 = nn.Conv2d(256, n_boxes['conv9_2'] * n_classes, kernel_size=3, padding=1)\n",
    "        self.cl_conv10_2 = nn.Conv2d(256, n_boxes['conv10_2'] * n_classes, kernel_size=3, padding=1)\n",
    "        self.cl_conv11_2 = nn.Conv2d(256, n_boxes['conv11_2'] * n_classes, kernel_size=3, padding=1)\n",
    "        \n",
    "        # initialize the weights\n",
    "        \n",
    "        self.init_conv2d()\n",
    "        \n",
    "    def init_conv2d(self):\n",
    "        \"\"\"\n",
    "        Simple function to initialize weights\n",
    "        \"\"\"\n",
    "        for c in self.children():\n",
    "            if isinstance(c, nn.Conv2d):\n",
    "                nn.init.xavier_uniform_(c.weight)\n",
    "                nn.init.constant_(c.bias, 0.)\n",
    "                \n",
    "    \n",
    "    # let's define the forward functions:\n",
    "    \n",
    "    def forward(self, conv8_2, conv9_2, conv10_2, conv11_2):\n",
    "        \"\"\"\n",
    "        Takes in the layers we defined in axuiliary convolutions and forwards them while getting the boxes outputs too\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        batch_size = conv8_2_feats.size(0)   # just taking a number from the feature map output by conv8_2 layer\n",
    "        # seems like we want the number of the boxes output by the conv8_2 layer to be the same \n",
    "        \n",
    "        # let's start with the first layer of the auxiliary convolutions:\n",
    "        \n",
    "        # perform the convolution on the feature map of the layer conv8_2 and get the output\n",
    "        l_conv8_2 = self.loc_conv8_2(conv8_2_feats)    # Outputs: (N, 24, 10, 10)\n",
    "        # now transform that output to be the same as prior-box order:\n",
    "        # permute just switches the location of values in the tensor\n",
    "        l_conv8_2 = l_conv8_2.permute(0, 2, 3, 1).contiguous() # (N, 10, 10, 24)\n",
    "        # .contiguous()Returns a contiguous in memory tensor containing the same data as self tensor. \n",
    "        # If self tensor is already in the specified memory format, this function returns the self tensor.\n",
    "        l_conv8_2 = l_conv8_2.view(batch_size, -1, 4) # \n",
    "        \n",
    "        l_conv9_2 = self.loc_conv8_2(conv9_2_feats)    # Outputs: (N, 24, 10, 10)\n",
    "        l_conv9_2 = l_conv9_2.permute(0, 2, 3, 1).contiguous() # (N, 10, 10, 24)\n",
    "        l_conv9_2 = l_conv9_2.view(batch_size, -1, 4) # (N, 150, 4)\n",
    "        \n",
    "        l_conv10_2 = self.loc_conv8_2(conv10_2_feats)    # Outputs: (N, 24, 10, 10)\n",
    "        l_conv10_2 = l_conv10_2.permute(0, 2, 3, 1).contiguous() # (N, 10, 10, 24)\n",
    "        l_conv10_2 = l_conv10_2.view(batch_size, -1, 4) # (N, 36, 4)\n",
    "        \n",
    "        l_conv11_2 = self.loc_conv8_2(conv11_2_feats)    # Outputs: (N, 24, 10, 10)\n",
    "        l_conv11_2 = l_conv11_2.permute(0, 2, 3, 1).contiguous() # (N, 10, 10, 24)\n",
    "        l_conv11_2 = l_conv11_2.view(batch_size, -1, 4) # (N, 4, 4)\n",
    "        \n",
    "        # Classification\n",
    "        # Predict classes for boxes i.e. Classification\n",
    "        c_conv8_2 = self.cl_conv8_2(conv8_2_feats)\n",
    "        c_conv8_2 = c_conv8_2.permute(0, 2, 3, 1).contiguous()\n",
    "        c_conv8_2 = c_conv8_2.view(batch_size, -1, self.n_classes)\n",
    "        \n",
    "        c_conv9_2 = self.cl_conv9_2(conv9_2_feats)\n",
    "        c_conv9_2 = c_conv9_2.permute(0, 2, 3, 1).contiguous()\n",
    "        c_conv9_2 = c_conv9_2.view(batch_size, -1, self.n_classes)\n",
    "        \n",
    "        c_conv10_2 = self.cl_conv10_2(conv10_2_feats)\n",
    "        c_conv10_2 = c_conv10_2.permute(0, 2, 3, 1).contiguous()\n",
    "        c_conv10_2 = c_conv10_2.view(batch_size, -1, self.n_classes)\n",
    "        \n",
    "        c_conv11_2 = self.cl_conv11_2(conv11_2_feats)\n",
    "        c_conv11_2 = c_conv11_2.permute(0, 2, 3, 1).contiguous()\n",
    "        c_conv11_2 = c_conv11_2.view(batch_size, -1, self.n_classes)\n",
    "        \n",
    "        locs = torch.cat([l_conv8_2, l_conv9_2, l_conv10_2, l_conv11_2], dim=1)\n",
    "        class_scores = torch.cat([c_conv8_2, c_conv9_2, c_conv10_2, c_conv11_2])\n",
    "        \n",
    "        return locs, class_scores\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now comes the fun part where we try to get the network put together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Squeeze_detect(nn.Module):\n",
    "    \"\"\"\n",
    "    squeeze_detect network that encapsulates the base Squeezenet, auxiliary and prediction convs\n",
    "    \"\"\"\n",
    "    def __init__(self, n_classes):\n",
    "        super(Squeeze_detect, self).__init__()\n",
    "        self.n_classes = n_classes\n",
    "        \n",
    "        self.base = SqueezeNet()\n",
    "        self.aux_convs = AuxiliaryConvolutions()\n",
    "        self.pred_convs = PredictionConvolutions(n_classes)\n",
    "        \n",
    "        self.rescale_factors = nn.Parameter(torch.FloatTensor(1, 512, 1, 1))    # there are 512 channels in 8_2 \n",
    "        nn.init.constant_(self.rescale_factors, 20)\n",
    "        \n",
    "        # prior boxes \n",
    "        self.prior_cxcy = self.create_prior_boxes()\n",
    "        \n",
    "    def forward(self, image):\n",
    "        \"\"\"\n",
    "        Forward Propagation.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Do not need to do these because we are not really retraining stuff from squeezenet\n",
    "       # conv8_2_feats = self.base(image)\n",
    "        \n",
    "        # rescale the conv8_2 after L2 norm\n",
    "       # norm = conv8_2_feats.pow(2).sum(dim=1, keepdim=True).sqrt()\n",
    "        # conv8_2_feats = conv8_2_feats / norm\n",
    "        # conv8_2_feats = conv8_3_feats * self.rescale_factors\n",
    "        \n",
    "        # Auxiliary\n",
    "        # higher level feature map generators\n",
    "        # HERE it would be something like SqueezeNet.features[12].expand1x1.weights?\n",
    "        conv8_2_feats, conv9_2_feats, conv10_2_feats, conv11_2_feats = self.aux_convs(conv7_feats)\n",
    "        \n",
    "        # run prediction convolutions\n",
    "        locs, classes_scores = self.pred_convs(conv8_2_feats, conv9_2_feats, conv10_2_feats, conv11_2_feats)\n",
    "        \n",
    "        return locs, class_scores\n",
    "    \n",
    "    def create_prior_boxes(self):\n",
    "        \"\"\"\n",
    "        create the ~ 8K numbers of priors but they would be less because we have less layers that we are doing\n",
    "        calcs on\n",
    "        \"\"\"\n",
    "        \n",
    "        # the feature map dimensions for each layer, we want the end \n",
    "        fmap_dims = {'conv8_2': 10,\n",
    "                     'conv9_2':5,\n",
    "                     'conv10_2':3,\n",
    "                     'conv11_2':1\n",
    "                    }\n",
    "    \n",
    "        obj_scales = {'conv8_2':0.375,\n",
    "                      'conv9_2':0.55,\n",
    "                      'conv10_2':0.725,\n",
    "                      'conv11_2':0.9                     \n",
    "                     }\n",
    "        aspect_ratios = {'conv8_2': [1., 2., 3., 0.5, .333],\n",
    "                         'conv9_2':[1., 2., 3., 0.5, .333],\n",
    "                         'conv10_2':[1., 2., 0.5],\n",
    "                         'conv11_2':[1., 2., 0.5]}\n",
    "    \n",
    "        \n",
    "        fmaps = list(fmap_dims.keys())\n",
    "        \n",
    "        prior_boxes = []\n",
    "        \n",
    "        for k, fmap in enumerate(fmaps):\n",
    "            for i in range(fmaps_dims[fmap]):\n",
    "                for j in range(fmaps_dims[fmap]):\n",
    "                    cx = (j + 0.5) / fmap_dims[fmap]\n",
    "                    cy = (i + 0.5) / fmap_dims[fmap]\n",
    "                    \n",
    "                    for ratio in aspect_ratios[fmap]:\n",
    "                        prior_boxes.append([cx, cy, obj_scales[fmap] * sqrt(ratio), obj_scales[fmap] /sqrt(ratio)])\n",
    "                        \n",
    "                        if ratio == 1:\n",
    "                            try:\n",
    "                                additional_scale = sqrt(obj_scales[fmap] * obj_scales[fmaps[k+1]])\n",
    "                                \n",
    "                            except:\n",
    "                                additional_scale = 1.\n",
    "                                prior_boxes.append([cx, cy, additional_scale, additional_scale])\n",
    "                                \n",
    "        prior_boxes = torch.FloatTensor(prior_boxes).to(device)\n",
    "        prior_boxes.clamp_(0, 1)\n",
    "        \n",
    "        return prior_boxes\n",
    "    \n",
    "    \n",
    "    def detect_objects(self, predicted_loss, predicted_scores, min_sccore, max_overlap, top_k):\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        pass\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "        fmap_dims = {'conv8_2': 10,\n",
    "                     'conv9_2':5,\n",
    "                     'conv10_2':3,\n",
    "                     'conv11_2':1\n",
    "                    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "    fmaps = list(fmap_dims.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "conv8_2\n",
      "1\n",
      "conv9_2\n",
      "2\n",
      "conv10_2\n",
      "3\n",
      "conv11_2\n"
     ]
    }
   ],
   "source": [
    "for k, fmap in enumerate(fmaps):\n",
    "    print(k)\n",
    "    print(fmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl = PredictionConvolutions(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we put nn.ReLU in between the conv layers, we would call on nn.Module and type:\n",
    "nn.ReLU(inplace=True)\n",
    "\n",
    "BUT if we put in the forward function, we would call on the ReLU function directly and "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IncompatibleKeys(missing_keys=[], unexpected_keys=[])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('squeezenet1_0-a815701f.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SqueezeNet(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 96, kernel_size=(7, 7), stride=(2, 2))\n",
       "    (1): ReLU(inplace)\n",
       "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "    (3): Fire(\n",
       "      (squeeze): Conv2d(96, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace)\n",
       "      (expand1x1): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace)\n",
       "      (expand3x3): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace)\n",
       "    )\n",
       "    (4): Fire(\n",
       "      (squeeze): Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace)\n",
       "      (expand1x1): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace)\n",
       "      (expand3x3): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace)\n",
       "    )\n",
       "    (5): Fire(\n",
       "      (squeeze): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace)\n",
       "      (expand1x1): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace)\n",
       "      (expand3x3): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace)\n",
       "    )\n",
       "    (6): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "    (7): Fire(\n",
       "      (squeeze): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace)\n",
       "      (expand1x1): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace)\n",
       "      (expand3x3): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace)\n",
       "    )\n",
       "    (8): Fire(\n",
       "      (squeeze): Conv2d(256, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace)\n",
       "      (expand1x1): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace)\n",
       "      (expand3x3): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace)\n",
       "    )\n",
       "    (9): Fire(\n",
       "      (squeeze): Conv2d(384, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace)\n",
       "      (expand1x1): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace)\n",
       "      (expand3x3): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace)\n",
       "    )\n",
       "    (10): Fire(\n",
       "      (squeeze): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace)\n",
       "      (expand1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace)\n",
       "      (expand3x3): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace)\n",
       "    )\n",
       "    (11): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "    (12): Fire(\n",
       "      (squeeze): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace)\n",
       "      (expand1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace)\n",
       "      (expand3x3): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace)\n",
       "    )\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.5)\n",
       "    (1): Conv2d(512, 1000, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (2): ReLU(inplace)\n",
       "    (3): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DETR model head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the squeezenet backbone, passing the image through the CNN, we pass in the feature pyramid to transformer encoder-decoder, which outputs a box of predictions:\n",
    "\n",
    "1. CNN Backbone:\n",
    "    - Input: Image\n",
    "    - Output: Feature maps with multiple channels that are flattened to be passed into the encoder\n",
    "2. Transformer encoder-decoder:\n",
    "    - Encoder takes in the flattened layer and outputs the same size shape sequence.\n",
    "    - Decoder takes in the output from the encoder but also takes in Object Queries\n",
    "    #### Object Queries\n",
    "        - Big N, represents the output we want i.e. if 4 tuples for 4 objects and their bounding boxes, we get that.\n",
    "        - Each object query vector wouldask the image different questions like what is in your right box, left box, lower left box, lower right box.\n",
    "3. Feed Forwad Network (FFN):\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
